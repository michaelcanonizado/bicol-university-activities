{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpSnJnfMmG7T",
        "outputId": "435e0c47-0c6c-48eb-d3a2-119429528e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# INSTALL DEPENDENCIES\n",
        "# ================================\n",
        "# pip install nltk spacy --quiet\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import spacy\n",
        "\n",
        "# Purpose:\n",
        "# • Stopwords = common words (a, the, is...) normally removed because they add no meaning.\n",
        "# • Lemmatizer = reduces words to dictionary form (better for meaning).\n",
        "# • Stemmer = cuts off word endings (faster but less accurate).\n",
        "\n",
        "# Load spacy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# ================================\n",
        "# SAMPLE DATASET\n",
        "# ================================\n",
        "texts = [\n",
        "    \"Natural Language Processing (NLP) is AMAZING!!!\",\n",
        "    \"Python provides powerful tools for text cleaning & tokenization.\",\n",
        "    \"Students should complete the pre-processing tasks on time.\"\n",
        "]\n",
        "\n",
        "print(\"Original Texts:\")\n",
        "for t in texts:\n",
        "    print(\"-\", t)\n",
        "\n",
        "# ===============================\n",
        "# PREPROCESSING FUNCTIONS\n",
        "# ===============================\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# What this does:\n",
        "# • Removes everything not a letter or whitespace\n",
        "#    → punctuation, numbers, symbols, emojis.\n",
        "# Example:\n",
        "# \"Hello!!! I'm 21 years old :)\" → \"Hello Im years old\"\n",
        "\n",
        "# Why important:\n",
        "# This simplifies text for models that expect alphabetic tokens only.\n",
        "\n",
        "def preprocess(text):\n",
        "    print(\"\\n===================================================\")\n",
        "    print(\"Original:\", text)\n",
        "\n",
        "    # --- 1. DATA CLEANING ---\n",
        "    cleaned = clean_text(text)\n",
        "    print(\"Cleaned:\", cleaned)\n",
        "    # Removes punctuation, digits, emojis, etc.\n",
        "\n",
        "    # --- 2. TOKENIZATION ---\n",
        "    tokens = word_tokenize(cleaned)\n",
        "    print(\"Tokens:\", tokens)\n",
        "    # Splits the sentence into individual words.\n",
        "\n",
        "    # --- 3. LOWERCASE ---\n",
        "    lower_tokens = [t.lower() for t in tokens]\n",
        "    print(\"Lowercased:\", lower_tokens)\n",
        "    # Models treat Cat # cat, so everything must be consistent.\n",
        "\n",
        "    # --- 4. STOP WORDS REMOVAL ---\n",
        "    no_stop = [t for t in lower_tokens if t not in stop_words]\n",
        "    print(\"No Stop Words:\", no_stop)\n",
        "    # ✓ Removes meaningless words like: the, is, am, are, to, of, in, that, this, etc.\n",
        "    # ✓ This reduces noise and improves algorithm performance.\n",
        "\n",
        "    # --- 5. LEMMATIZATION ---\n",
        "    lemmas = [lemmatizer.lemmatize(t) for t in no_stop]\n",
        "    print(\"Lemmatized:\", lemmas)\n",
        "\n",
        "    # Converts words to their dictionary/root form.\n",
        "    # Examples:\n",
        "    # • \"cars\" → \"car\"\n",
        "    # • \"running\" → \"run\"\n",
        "    # • \"better\" → \"good\"\n",
        "    # ✓ Improves generalization for ML models.\n",
        "\n",
        "    # --- 6. STEMMING ---\n",
        "    stems = [stemmer.stem(t) for t in no_stop]\n",
        "    print(\"Stemmed:\", stems)\n",
        "\n",
        "    # Cuts words to their root form:\n",
        "    # • \"playing\" → \"play\"\n",
        "    # • \"studies\" → \"studi\"\n",
        "    # • \"better\" → \"better\" (unchanged)\n",
        "    # ✓ Good for search engines.\n",
        "    # ✓ NOT always good for ML because it can distort meaning.\n",
        "\n",
        "    # --- 7. POS TAGGING ---\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "    print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "    # Uses spaCy to get:\n",
        "    # • Noun\n",
        "    # • Verb\n",
        "    # • Adjective\n",
        "    # • Adverb\n",
        "    # • Proper noun\n",
        "    # • Determiner\n",
        "    # • Pronoun\n",
        "    # • ... etc.\n",
        "\n",
        "# ---\n",
        "# APPLY TO ALL TEXTS\n",
        "# ---\n",
        "for t in texts:\n",
        "    preprocess(t)\n",
        "# Feature Extraction\n",
        "\n",
        "# ================================\n",
        "# INSTALL REQUIRED LIBRARIES\n",
        "# ================================\n",
        "# pip install nltk gensim scikit-learn --quiet\n",
        "\n",
        "#     nltk → tokenization\n",
        "#     gensim → Word2Vec\n",
        "#     scikit-learn → BoW, TF-IDF\n",
        "#     pandas → convert matrices to DataFrames\n",
        "\n",
        "# IMPORTS\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "\n",
        "# ================================\n",
        "# SAMPLE DATASET\n",
        "# ================================\n",
        "texts = [\n",
        "    \"Natural Language Processing enables computers to understand text.\",\n",
        "    \"Machine learning provides tools for analyzing large volumes of data.\",\n",
        "    \"Deep learning models require a lot of high quality training data.\"\n",
        "]\n",
        "\n",
        "# ================================\n",
        "# BAG OF WORDS\n",
        "# ================================\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(texts)\n",
        "\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(),\n",
        "                      columns=vectorizer.get_feature_names_out())\n",
        "print(\"==B A G  O F  W O R D S ==\")\n",
        "print(bow_df)\n",
        "\n",
        "# ✓ Builds a vocabulary of all unique words.\n",
        "# ✓ Counts how many times each word appears in every sentence.\n",
        "\n",
        "# Good for:\n",
        "# • Traditional ML algorithms (SVM, LR, Naive Bayes)\n",
        "# • Simple text classification\n",
        "\n",
        "# Limitations:\n",
        "# • Ignores grammar & order\n",
        "# • High-dimensional (many columns)\n",
        "\n",
        "# ================================================================\n",
        "# N-GRAMS (Unigrams, Bigrams, Trigrams)\n",
        "# ================================================================\n",
        "ngram_vectorizer = CountVectorizer(ngram_range=(1,3))\n",
        "ngram_matrix = ngram_vectorizer.fit_transform(texts)\n",
        "\n",
        "ngram_df = pd.DataFrame(ngram_matrix.toarray(),\n",
        "                        columns=ngram_vectorizer.get_feature_names_out())\n",
        "print(\"\\n=== N - G R A M S (1 to 3) ===\")\n",
        "print(ngram_df)\n",
        "\n",
        "# Creates features for:\n",
        "# • Unigrams → 1 word: \"learning\"\n",
        "# • Bigrams → 2 words: \"machine learning\"\n",
        "# • Trigrams → 3 words: \"high quality training\"\n",
        "\n",
        "# ✓ Captures context and meaning better than BoW.\n",
        "\n",
        "# Example:\n",
        "# • Method -> \"not good\" meaning\n",
        "# • Unigram -> \"not\" + \"good\" (can't detect negativity)\n",
        "# • Bigram -> \"not good\" (captures negative sentiment)\n",
        "\n",
        "# Use cases:\n",
        "# • Sentiment analysis\n",
        "# • Spam detection\n",
        "# • Intent classification\n",
        "\n",
        "# ================================================================\n",
        "# TF-IDF\n",
        "# ================================================================\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(texts)\n",
        "\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(),\n",
        "                        columns=tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"\\n=== T F - I D F ===\")\n",
        "print(tfidf_df)\n",
        "\n",
        "# Term Frequency × Inverse Document Frequency\n",
        "# • Words that appear many times in one document → high weight\n",
        "# • Words that appear in all documents → low weight (not useful)\n",
        "# TF-IDF is better than BoW:\n",
        "# - Reduces weight of common words like \"data,\" \"learning\"\n",
        "# - Highlights important keywords\n",
        "\n",
        "# Example weights (illustrative):\n",
        "# | Term    | Text 1 | Text 2 | Text 3 |\n",
        "# |---------|--------|--------|--------|\n",
        "# | processing | 0.7   | 0.0   | 0.0   |\n",
        "# | learning | 0.0   | 0.6   | 0.6   |\n",
        "# | data    | 0.0   | 0.3   | 0.4   |\n",
        "\n",
        "# ================================================================\n",
        "# WORD2VEC\n",
        "# ================================================================\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "\n",
        "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=50, window=5, min_count=1, workers=4)\n",
        "\n",
        "print(\"\\n=== W O R D 2 V E C : Vocabulary ===\")\n",
        "print(list(w2v_model.wv.index_to_key))\n",
        "\n",
        "print(\"\\n=== Vector for word 'data' ===\")\n",
        "print(w2v_model.wv[\"data\"])\n",
        "\n",
        "print(\"\\n=== Most similar to 'data' ===\")\n",
        "print(w2v_model.wv.most_similar(\"data\"))\n",
        "\n",
        "# Word2Vec does:\n",
        "# - Learns semantic meaning of words by looking at context.\n",
        "# - Instead of counts, each word becomes a dense numeric vector (e.g., 50 dimensions).\n",
        "# - Meaning:\n",
        "#   - \"data\" appears in similar contexts as \"training,\" \"volumes,\" etc.\n",
        "\n",
        "# **Strengths:**\n",
        "# - Captures meaning\n",
        "# - Maintains relations (king - man + woman = queen)\n",
        "# - Useful for deep learning\n",
        "\n",
        "# **Weaknesses:**\n",
        "# - Needs many sentences to train well\n",
        "# - Models trained on small corpus may be weak"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZb7YTFdmwAh",
        "outputId": "010c7ae1-2317-4608-b932-5b8f24aa7c32"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Texts:\n",
            "- Natural Language Processing (NLP) is AMAZING!!!\n",
            "- Python provides powerful tools for text cleaning & tokenization.\n",
            "- Students should complete the pre-processing tasks on time.\n",
            "\n",
            "===================================================\n",
            "Original: Natural Language Processing (NLP) is AMAZING!!!\n",
            "Cleaned: Natural Language Processing NLP is AMAZING\n",
            "Tokens: ['Natural', 'Language', 'Processing', 'NLP', 'is', 'AMAZING']\n",
            "Lowercased: ['natural', 'language', 'processing', 'nlp', 'is', 'amazing']\n",
            "No Stop Words: ['natural', 'language', 'processing', 'nlp', 'amazing']\n",
            "Lemmatized: ['natural', 'language', 'processing', 'nlp', 'amazing']\n",
            "Stemmed: ['natur', 'languag', 'process', 'nlp', 'amaz']\n",
            "POS Tags: [('Natural', 'PROPN'), ('Language', 'PROPN'), ('Processing', 'PROPN'), ('NLP', 'PROPN'), ('is', 'AUX'), ('AMAZING', 'ADJ')]\n",
            "\n",
            "===================================================\n",
            "Original: Python provides powerful tools for text cleaning & tokenization.\n",
            "Cleaned: Python provides powerful tools for text cleaning  tokenization\n",
            "Tokens: ['Python', 'provides', 'powerful', 'tools', 'for', 'text', 'cleaning', 'tokenization']\n",
            "Lowercased: ['python', 'provides', 'powerful', 'tools', 'for', 'text', 'cleaning', 'tokenization']\n",
            "No Stop Words: ['python', 'provides', 'powerful', 'tools', 'text', 'cleaning', 'tokenization']\n",
            "Lemmatized: ['python', 'provides', 'powerful', 'tool', 'text', 'cleaning', 'tokenization']\n",
            "Stemmed: ['python', 'provid', 'power', 'tool', 'text', 'clean', 'token']\n",
            "POS Tags: [('Python', 'PROPN'), ('provides', 'VERB'), ('powerful', 'ADJ'), ('tools', 'NOUN'), ('for', 'ADP'), ('text', 'NOUN'), ('cleaning', 'NOUN'), ('tokenization', 'NOUN')]\n",
            "\n",
            "===================================================\n",
            "Original: Students should complete the pre-processing tasks on time.\n",
            "Cleaned: Students should complete the preprocessing tasks on time\n",
            "Tokens: ['Students', 'should', 'complete', 'the', 'preprocessing', 'tasks', 'on', 'time']\n",
            "Lowercased: ['students', 'should', 'complete', 'the', 'preprocessing', 'tasks', 'on', 'time']\n",
            "No Stop Words: ['students', 'complete', 'preprocessing', 'tasks', 'time']\n",
            "Lemmatized: ['student', 'complete', 'preprocessing', 'task', 'time']\n",
            "Stemmed: ['student', 'complet', 'preprocess', 'task', 'time']\n",
            "POS Tags: [('Students', 'NOUN'), ('should', 'AUX'), ('complete', 'VERB'), ('the', 'DET'), ('preprocessing', 'VERB'), ('tasks', 'NOUN'), ('on', 'ADP'), ('time', 'NOUN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==B A G  O F  W O R D S ==\n",
            "   analyzing  computers  data  deep  enables  for  high  language  large  \\\n",
            "0          0          1     0     0        1    0     0         1      0   \n",
            "1          1          0     1     0        0    1     0         0      1   \n",
            "2          0          0     1     1        0    0     1         0      0   \n",
            "\n",
            "   learning  ...  processing  provides  quality  require  text  to  tools  \\\n",
            "0         0  ...           1         0        0        0     1   1      0   \n",
            "1         1  ...           0         1        0        0     0   0      1   \n",
            "2         1  ...           0         0        1        1     0   0      0   \n",
            "\n",
            "   training  understand  volumes  \n",
            "0         0           1        0  \n",
            "1         0           0        1  \n",
            "2         1           0        0  \n",
            "\n",
            "[3 rows x 25 columns]\n",
            "\n",
            "=== N - G R A M S (1 to 3) ===\n",
            "   analyzing  analyzing large  analyzing large volumes  computers  \\\n",
            "0          0                0                        0          1   \n",
            "1          1                1                        1          0   \n",
            "2          0                0                        0          0   \n",
            "\n",
            "   computers to  computers to understand  data  deep  deep learning  \\\n",
            "0             1                        1     0     0              0   \n",
            "1             0                        0     1     0              0   \n",
            "2             0                        0     1     1              1   \n",
            "\n",
            "   deep learning models  ...  tools  tools for  tools for analyzing  training  \\\n",
            "0                     0  ...      0          0                    0         0   \n",
            "1                     0  ...      1          1                    1         0   \n",
            "2                     1  ...      0          0                    0         1   \n",
            "\n",
            "   training data  understand  understand text  volumes  volumes of  \\\n",
            "0              0           1                1        0           0   \n",
            "1              0           0                0        1           1   \n",
            "2              1           0                0        0           0   \n",
            "\n",
            "   volumes of data  \n",
            "0                0  \n",
            "1                1  \n",
            "2                0  \n",
            "\n",
            "[3 rows x 72 columns]\n",
            "\n",
            "=== T F - I D F ===\n",
            "   analyzing  computers      data      deep   enables       for      high  \\\n",
            "0   0.000000   0.353553  0.000000  0.000000  0.353553  0.000000  0.000000   \n",
            "1   0.338348   0.000000  0.257322  0.000000  0.000000  0.338348  0.000000   \n",
            "2   0.000000   0.000000  0.257322  0.338348  0.000000  0.000000  0.338348   \n",
            "\n",
            "   language     large  learning  ...  processing  provides   quality  \\\n",
            "0  0.353553  0.000000  0.000000  ...    0.353553  0.000000  0.000000   \n",
            "1  0.000000  0.338348  0.257322  ...    0.000000  0.338348  0.000000   \n",
            "2  0.000000  0.000000  0.257322  ...    0.000000  0.000000  0.338348   \n",
            "\n",
            "    require      text        to     tools  training  understand   volumes  \n",
            "0  0.000000  0.353553  0.353553  0.000000  0.000000    0.353553  0.000000  \n",
            "1  0.000000  0.000000  0.000000  0.338348  0.000000    0.000000  0.338348  \n",
            "2  0.338348  0.000000  0.000000  0.000000  0.338348    0.000000  0.000000  \n",
            "\n",
            "[3 rows x 25 columns]\n",
            "\n",
            "=== W O R D 2 V E C : Vocabulary ===\n",
            "['.', 'data', 'of', 'learning', 'training', 'quality', 'high', 'lot', 'a', 'require', 'models', 'deep', 'volumes', 'large', 'analyzing', 'for', 'tools', 'provides', 'machine', 'text', 'understand', 'to', 'computers', 'enables', 'processing', 'language', 'natural']\n",
            "\n",
            "=== Vector for word 'data' ===\n",
            "[-0.01633495  0.00897995 -0.00824606  0.00163228  0.01700144 -0.00891102\n",
            "  0.00902947 -0.0135547  -0.00708671  0.01878738 -0.00315548  0.00062026\n",
            " -0.00830066 -0.01535872 -0.00302007  0.00494083 -0.00177605  0.01108998\n",
            " -0.00548326  0.0045097   0.01091003  0.01669268 -0.0028931  -0.01840909\n",
            "  0.00874392  0.00116379  0.01487311 -0.00161215 -0.00528128 -0.01749321\n",
            " -0.00172088  0.00565012  0.01079692  0.01409941 -0.01141119  0.00370122\n",
            "  0.01217665 -0.00959398 -0.00619676  0.0136024   0.00327298  0.00038353\n",
            "  0.00692654  0.00043492  0.01922178  0.0101153  -0.01781625 -0.01410135\n",
            "  0.00179786  0.01278798]\n",
            "\n",
            "=== Most similar to 'data' ===\n",
            "[('tools', 0.2296695113182068), ('language', 0.2190629243850708), ('text', 0.1603885143995285), ('machine', 0.14832599461078644), ('training', 0.12472919374704361), ('volumes', 0.0805644616484642), ('high', 0.07392346113920212), ('to', 0.05556977540254593), ('.', 0.04245469346642494), ('quality', 0.018196923658251762)]\n"
          ]
        }
      ]
    }
  ]
}